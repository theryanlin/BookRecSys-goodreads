{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_csv('goodbooks-10k-dataset/ratings.csv')\n",
    "tr = pd.read_csv('goodbooks-10k-dataset/to_read.csv')\n",
    "b = pd.read_csv('goodbooks-10k-dataset/books.csv')\n",
    "\n",
    "t = pd.read_csv('goodbooks-10k-dataset/tags.csv')\n",
    "bt = pd.read_csv('goodbooks-10k-dataset/book_tags.csv')\n",
    "bg = pd.read_csv('goodbooks-10k-dataset/book_genre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = bg.groupby('goodreads_book_id')['genre'].apply(lambda x: ' '.join(set(x))).reset_index()\n",
    "\n",
    "# Combine book features\n",
    "b = b.merge(books_df, how='left').fillna('')\n",
    "b['content'] = b['title'] + ' ' + b['authors'] + ' ' + b['original_title'] + ' ' + b['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the books data\n",
    "books_df = b[['goodreads_book_id', 'title', 'authors', 'original_title', 'genre']].copy()\n",
    "books_df = books_df.rename(columns={'goodreads_book_id': 'book_id'})\n",
    "books_df['content'] = books_df['title'] + ' ' + books_df['authors'] + ' ' + books_df['original_title'] + ' ' + books_df['genre']\n",
    "\n",
    "# Prepare ratings data\n",
    "ratings = r[['user_id', 'book_id', 'rating']].copy()\n",
    "\n",
    "# Split into train and test (80/20)\n",
    "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split test into validation and test if needed\n",
    "# val_ratings, test_ratings = train_test_split(test_ratings, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(books_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def item_content_based_recommendation(books_df, ratings_df, user_id, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Item content-based filtering using TF-IDF on genres/tags.\n",
    "\n",
    "    Args:\n",
    "        books_df: DataFrame (book_id, content)\n",
    "        r: Ratings DataFrame (user_id, book_id, rating)\n",
    "        user_id: Target user\n",
    "        n_recommendations: Number of results\n",
    "\n",
    "    Returns:\n",
    "        List of recommended book IDs\n",
    "    \"\"\"\n",
    "\n",
    "    book_ids = books_df['book_id'].values\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    sim_df = pd.DataFrame(sim_matrix, index=book_ids, columns=book_ids)\n",
    "\n",
    "    user_ratings = ratings_df[ratings_df['user_id'] == user_id]\n",
    "    books_rated = user_ratings['book_id'].values\n",
    "    ratings_given = user_ratings.set_index('book_id')['rating']\n",
    "\n",
    "    scores = {}\n",
    "    for book in books_rated:\n",
    "        if book not in sim_df.index:\n",
    "            continue\n",
    "        similar_books = sim_df[book]\n",
    "        for other_book, sim_score in similar_books.items():\n",
    "            if other_book in books_rated:\n",
    "                continue\n",
    "            scores.setdefault(other_book, 0)\n",
    "            scores[other_book] += sim_score * ratings_given.get(book, 0)\n",
    "\n",
    "    ranked = pd.Series(scores).sort_values(ascending=False)\n",
    "    return ranked.head(n_recommendations).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_recommendations(test_ratings, recommendations, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations against test set\n",
    "    \n",
    "    Args:\n",
    "        test_ratings: DataFrame with true user-item ratings\n",
    "        recommendations: Dict {user_id: list of recommended book_ids}\n",
    "        k: Top-k recommendations to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Create a test set of user-item pairs with ratings >= threshold (e.g., 4)\n",
    "    test_positives = test_ratings[test_ratings['rating'] >= 4]\n",
    "    test_dict = test_positives.groupby('user_id')['book_id'].apply(list).to_dict()\n",
    "    \n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    for user_id, recs in recommendations.items():\n",
    "        if user_id not in test_dict:\n",
    "            continue\n",
    "            \n",
    "        true_positives = test_dict[user_id]\n",
    "        if not true_positives:\n",
    "            continue\n",
    "            \n",
    "        # Get top-k recommendations\n",
    "        top_k_recs = recs[:k]\n",
    "        \n",
    "        # Calculate hits\n",
    "        hits = len(set(top_k_recs) & set(true_positives))\n",
    "        \n",
    "        # Precision@k\n",
    "        precision = hits / k\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        # Recall@k\n",
    "        recall = hits / len(true_positives)\n",
    "        recall_scores.append(recall)\n",
    "        \n",
    "        # NDCG@k (we'll use binary relevance here)\n",
    "        relevance = [1 if book in true_positives else 0 for book in top_k_recs]\n",
    "        ideal_relevance = sorted(relevance, reverse=True)\n",
    "        if sum(ideal_relevance) > 0:\n",
    "            ndcg = ndcg_score([ideal_relevance], [relevance], k=k)\n",
    "            ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return {\n",
    "        'precision@k': np.mean(precision_scores) if precision_scores else 0,\n",
    "        'recall@k': np.mean(recall_scores) if recall_scores else 0,\n",
    "        'ndcg@k': np.mean(ndcg_scores) if ndcg_scores else 0,\n",
    "        'coverage': len(recommendations) / test_ratings['user_id'].nunique(),\n",
    "        'num_users_evaluated': len(precision_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations:   0%|                      | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 5000 users with both train and test ratings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|█████████| 5000/5000 [5:13:28<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Precision@5: 0.0246\n",
      "Recall@5: 0.0163\n",
      "NDCG@5: 0.4451\n",
      "Coverage: 9.36%\n",
      "Users evaluated: 4996\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Randomly sample 5000 users from test set (excluding users with no train ratings)\n",
    "test_users = test_ratings['user_id'].unique()\n",
    "train_users = set(train_ratings['user_id'].unique())\n",
    "\n",
    "# Only evaluate users that appear in both train and test\n",
    "valid_users = list(set(test_users) & train_users)\n",
    "sample_users = random.sample(valid_users, min(5000, len(valid_users)))\n",
    "\n",
    "print(f\"Evaluating on {len(sample_users)} users with both train and test ratings\")\n",
    "\n",
    "# Generate recommendations with progress bar\n",
    "recommendations = {}\n",
    "for user_id in tqdm(sample_users, desc=\"Generating recommendations\"):\n",
    "    try:\n",
    "        recs = item_content_based_recommendation(books_df, train_ratings, user_id, n_recommendations=10)\n",
    "        recommendations[user_id] = recs\n",
    "    except Exception as e:\n",
    "        # Uncomment below for debugging\n",
    "        # print(f\"Error for user {user_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "metrics = evaluate_recommendations(test_ratings, recommendations, k=10)\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Precision: {metrics['precision@k']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall@k']:.4f}\")\n",
    "print(f\"NDCG: {metrics['ndcg@k']:.4f}\")\n",
    "print(f\"Coverage: {metrics['coverage']:.2%}\")\n",
    "print(f\"Users evaluated: {metrics['num_users_evaluated']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
